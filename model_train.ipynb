{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install -q wordcloud\n",
    "!pip install matplotlib\n",
    "!pip install vaderSentiment\n",
    "!pip install NRCLex\n",
    "!pip install seaborn\n",
    "!pip install tensorflow\n",
    "!pip install keras\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import the Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6116dbf03b2a509"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pandas import DataFrame as df\n",
    "import matplotlib.pyplot as mp\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib as plt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f09a4900e1fa25f3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "from nrclex import NRCLex\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "df['fear'] = df['text'].apply(lambda x: NRCLex(x).raw_emotion_scores.get('fear', 0))\n",
    "\n",
    "df['anger'] = df['text'].apply(lambda x: NRCLex(x).raw_emotion_scores.get('anger', 0))\n",
    "\n",
    "df['anticipation'] = df['text'].apply(lambda x: NRCLex(x).raw_emotion_scores.get('anticipation', 0))\n",
    "\n",
    "df['trust'] = df['text'].apply(lambda x: NRCLex(x).raw_emotion_scores.get('trust', 0))\n",
    "\n",
    "df['surprise'] = df['text'].apply(lambda x: NRCLex(x).raw_emotion_scores.get('surprise', 0))\n",
    "\n",
    "df['positive'] = df['text'].apply(lambda x: NRCLex(x).raw_emotion_scores.get('positive', 0))\n",
    "\n",
    "df['negative'] = df['text'].apply(lambda x: NRCLex(x).raw_emotion_scores.get('negative', 0))\n",
    "\n",
    "df['sadness'] = df['text'].apply(lambda x: NRCLex(x).raw_emotion_scores.get('sadness', 0))\n",
    "\n",
    "df['disgust'] = df['text'].apply(lambda x: NRCLex(x).raw_emotion_scores.get('disgust', 0))\n",
    "\n",
    "df['joy'] = df['text'].apply(lambda x: NRCLex(x).raw_emotion_scores.get('joy', 0))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    sentence = row['text']\n",
    "    sentiment = analyzer.polarity_scores(sentence)\n",
    "\n",
    "    df.at[index, 'negative'] = sentiment['neg']\n",
    "    df.at[index, 'neutral'] = sentiment['neu']\n",
    "    df.at[index, 'positive'] = sentiment['pos']\n",
    "    df.at[index, 'compound'] = sentiment['compound']\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c99f74c0f2b6752",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['Message Size'] = df['text'].apply(lambda x: len(x))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7633f0825639800f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e45cbff25d6f3023",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df1 = df.copy();\n",
    "\n",
    "df1.drop(columns=['text'], inplace=True)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "# plotting correlation heatmap \n",
    "dataplot = sb.heatmap(df1.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "\n",
    "# displaying heatmap \n",
    "mp.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b1c41d36312a77a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_43 (Embedding)    (None, 36, 128)           256000    \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 36, 16)            2064      \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 36, 1)             17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 258081 (1008.13 KB)\n",
      "Trainable params: 258081 (1008.13 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.src.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.src.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.src.layers import Dense, Embedding\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df = pd.read_csv(\"./datasets/train2Data.csv\", encoding=\"utf-8\", sep=',')\n",
    "\n",
    "df.rename(columns={'Tweets': 'text', ' Label': 'label'}, inplace=True)\n",
    "\n",
    "#Remove stopwords from the dataset\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([item for item in x.split() if item not in ['amp', 'quot']]))\n",
    "\n",
    "\n",
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(df['text'].values)\n",
    "X = tokenizer.texts_to_sequences(df['text'].values)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "model.add(Dense(16))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,df['label'], test_size = 0.20, random_state = 42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T18:01:34.965813500Z",
     "start_time": "2024-03-05T18:01:30.727476700Z"
    }
   },
   "id": "caf5d99326c5b829"
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "775/775 [==============================] - 4s 5ms/step - loss: 0.6424 - accuracy: 0.5896 - val_loss: 0.6376 - val_accuracy: 0.5938\n",
      "Epoch 2/7\n",
      "775/775 [==============================] - 3s 4ms/step - loss: 0.6370 - accuracy: 0.5947 - val_loss: 0.6373 - val_accuracy: 0.5938\n",
      "Epoch 3/7\n",
      "775/775 [==============================] - 3s 4ms/step - loss: 0.6366 - accuracy: 0.5962 - val_loss: 0.6373 - val_accuracy: 0.5940\n",
      "Epoch 4/7\n",
      "775/775 [==============================] - 3s 4ms/step - loss: 0.6360 - accuracy: 0.5975 - val_loss: 0.6372 - val_accuracy: 0.5939\n",
      "Epoch 5/7\n",
      "775/775 [==============================] - 3s 4ms/step - loss: 0.6359 - accuracy: 0.5977 - val_loss: 0.6371 - val_accuracy: 0.5941\n",
      "Epoch 6/7\n",
      "775/775 [==============================] - 3s 4ms/step - loss: 0.6357 - accuracy: 0.5975 - val_loss: 0.6368 - val_accuracy: 0.5945\n",
      "Epoch 7/7\n",
      "775/775 [==============================] - 3s 4ms/step - loss: 0.6357 - accuracy: 0.5978 - val_loss: 0.6382 - val_accuracy: 0.5940\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x23808441950>"
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs = 7, validation_data=(X_test, Y_test),batch_size=32, verbose = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T18:02:00.448592100Z",
     "start_time": "2024-03-05T18:01:36.701749500Z"
    }
   },
   "id": "a035d0abd400c839"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
